{
    "native_identifier": "flash-linear-attention",
    "repo": "fla-org/flash-linear-attention",
    "registry": "pypi",
    "tag_formats": [
        "v{major}.{minor}.{patch}"
    ],
    "sentinel_timestamp": "2020-01-01T00:00:00Z",
    "include": [
        "**/*.md",
        "**/*.py"
    ],
    "collection_name_prefix": "flash-linear-attention",
    "version_sample_relative_size": 0.5,
    "version_sample_max_size": 5
}